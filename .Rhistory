all
all <- train(diagnosis~., data=new, method="rf")
confusionMatrix(predict(all, alltest), testing$diagnosis)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
?plot.enet
?lasso
??lasso
rm(list=ls())
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
str(training)
fit <- train(CompressiveStrength~., data=training, method="lasso")
?plot.enet
plot(fit)
plot.enet(fit)
plot(fit, xvar=c("penalty"))
plot(fit, xvar=c("fraction"))
plot(fit, xvar=c("penalty"))
data(diabetes)
attach(diabetes)
object <- enet(x,y,lambda=1)
par(mfrow=c(2,2))
plot(object)
plot(object,xvar="step")
detach(diabetes)
type.of
type
typeof
typeof(fit)
classof(fit)
class(fit)
class(object)
?enet
fit
names(fit)
fit$finalModel
class(fit$finalModel)
plot(fit$finalModel)
plot(fit$finalModel)
plot(fit$finalModel)
par(mfrow=c(1,1))
plot(fit$finalModel)
plot(fit$finalModel, xvar="penalty")
getwd()
?download.file
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv", "gaData.csv")
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(lubridate)  # For year() function below
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
?forecast
library(forecst)
library(forecast)
install.packages("forecast")
library(forecast)
?bats
bats(tstrain)
bat <- bats(tstrain)
bat
forecast(bat)
forecast(bat, testing)
forecast(bat, training)
forecast(bat)
rm(list=ls())
#Load the data on the number of visitors to the instructors blog from here:
# https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv
library(lubridate) # For year() function below
dat = read.csv("gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
# Fit a model using the bats() function in the forecast package to the training
# time series. Then forecast this model for the remaining time points. For how
# many of the testing points is the true value within the 95% prediction
# interval bounds?
library(forecast)
library(quantmod)
# fit a model
fit <- bats(tstrain)
# check how long the test set is, so you can predict beyond trainign
h <- dim(testing)[1]
# forecast the model for remaining time points
fcast <- forecast(fit, level = 95, h = h)
# get the accuracy
accuracy(fcast, testing$visitsTumblr)
# check what percentage of times that the actual number of visitors was within
# 95% confidence interval
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
rm(list=ls())
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 325 and fit a support vector machine using the e1071 package
# to predict Compressive Strength using the default settings. Predict on the
# testing set. What is the RMSE?
set.seed(325)
library(e1071)
library(caret)
fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")
prediction <- predict(fit, testing)
accuracy(prediction, testing$CompressiveStrength)
rm(list=ls())
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
# Set the seed to 233 and fit a lasso model to predict Compressive Strength.
# Which variable is the last coefficient to be set to zero as the penalty increases?
# (Hint: it may be useful to look up ?plot.enet).
library(caret)
set.seed(233)
fit <- train(CompressiveStrength ~ ., data = training, method = "lasso")
# Since we are interested in the shrinkage of coefficients as the penalty(lambda) increases, "
# penalty" looks promising for an xvar argument value.
plot.enet(fit$finalModel, xvar = "penalty", use.color = TRUE)
rm(list=ls())
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Set the seed to 62433 and predict diagnosis with all the other variables using
# a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis
# ("lda") model. Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set? Is it better or worse than
# each of the individual predictions?
set.seed(62433)
# create models
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fit3 <- train(diagnosis ~ ., data = training, method = "lda")
# predict test
predict1 <- predict(fit1, newdata = testing)
predict2 <- predict(fit2, newdata = testing)
predict3 <- predict(fit3, newdata = testing)
# combine predictions
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis) # training$diagnosis?
fit_combined <- train(diagnosis ~ ., data = DF_combined, method = "rf")
predict4 <- predict(fit_combined, newdata = testing)
# confusion matrixes
c1 <- confusionMatrix(predict1, testing$diagnosis)
c2 <- confusionMatrix(predict2, testing$diagnosis)
c3 <- confusionMatrix(predict3, testing$diagnosis)
c4 <- confusionMatrix(predict4, testing$diagnosis)
print(paste(c1$overall[1], c2$overall[1], c3$overall[1], c4$overall[1]))
c1
c2
c3
c4
rm(list=ls())
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
fit1 <- train(diagnosis ~ ., data = training, method = "rf")
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fit3 <- train(diagnosis ~ ., data = training, method = "lda")
predict1 <- predict(fit1, testing)
predict2 <- predict(fit2, testing)
predict3 <- predict(fit3, testing)
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis) # training$diagnosis?
fit_combined <- train(diagnosis ~ ., data = DF_combined, method = "rf")
predict4 <- predict(fit_combined, newdata = testing)
c1 <- confusionMatrix(predict1, testing$diagnosis)
c2 <- confusionMatrix(predict2, testing$diagnosis)
c3 <- confusionMatrix(predict3, testing$diagnosis)
c4 <- confusionMatrix(predict4, testing$diagnosis)
c1
c2
c3
c4
print(paste(c1$overall[1], c2$overall[1], c3$overall[1], c4$overall[1]))
n = 1000
t = 15000
x = rep(0, n)
y = rep(0, n)
?for
?'for'
for(i in 1:t){
for(j in 1:n){
x(j) = x(j) + rnorm()
y(j) = y(j) + rnorm()
}
}
x[1]
for(i in 1:t){
for(j in 1:n){
x[j] = x[j] + rnorm()
y[j] = y[j] + rnorm()
}
}
?rnom
?rnorm
for(i in 1:t){
x = x + rnorm(n)
y = y + rnorm(n)
}
mean(x)
mean(x**2)
(mean(x**2)-mean(x))/2
(mean(x**2)-mean(x)**2)/2
(mean(x**2)-mean(x)**2)/(2*t)
(mean(x**2)-mean(x)**2)/(t)
for(i in 1:t){
x = x + rnorm(n, 0, 5)
y = y + rnorm(n, 0, 5)
}
x = rep(0, n)
y = rep(0, n)
for(i in 1:t){
x = x + rnorm(n, 0, 5)
y = y + rnorm(n, 0, 5)
}
(mean(x**2)-mean(x)**2)/(t)
(mean(x**2)-mean(x)**2)/(2*t)
notes <- c(3.9, 1.0, 3.2, 9.0, 5.9, 7.0, 5.0, 2.3, 4.2, 5.1, 2.9, 3.7, 3.7, 4.0, 5.0, 4.3, 4.5, 4.2, 3.3, 5.5, 2.7, 5.4, 2.7, 7.2, 3.2, 3.3, 2.1, 5.0, 7.1, 3.2, 4.2, 9.6, 2.9, 6.4, 8.0, 5.6, 5.0, 8.7, 7.3, 7.5, 5.7, 7.1, 8.3, 4.2, 7.4, 6.3, 7.1, 8.0, 9.5, 4.2, 9.0, 7.0, 7.1, 7.2, 3.8, 7.0, 9.8, 3.7, 8.0, 5.2, 5.4, 9.2, 6.5, 4.0, 6.5, 6.6, 7.0, 8.1, 5.0, 8.0, 5.7)
?plot
summary(notes)
plot(notes, tupe='h')
plot(notes, type='h')
?plot
?hist
hist(notes)
?hist
hist(notes, breaks=10)
hist(notes)
hist(notes, breaks=20)
sum(notes < 5.0)
sum(notes > 5.0)
sum(notes > 9.0)
sum(notes >= 9.0)
sum(notes == 9.0)
sum(notes < 9.0)
sum(notes < 5.0)
sum(notes >= 5.0)
26/length(notes)
install.packages("shiny")
install.packages("manipulate")
library(shiny)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp(display.mode='showcase')
lst <- list(numbers = 1:10, letters = letters, boolean = c(TRUE, FALSE))
lst$numbers
type(lst$numbers)
typeof(lst$numbers)
class(lst$numbers)
lst[1]
typeof(lst[1])
lst[[1]]
typeof(lst[[1]])
install.package("rCharts")
install.packages("rCharts")
require(rCharts)
install.packages("rCharts")
library(devtools)
install.packages("devtools")
library(devtools)
install_github("ramnathv/rCharts@dev")
install.packages("base64enc")
install_github("ramnathv/rCharts@dev")
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(rCharts)
library(airquality)
galton
dataset?
help dataset
help(dataset)
?dataset
??dataset
library(datasets)
?datasets
library(help = "datasets")
load(airquality)
airquality
dTable(airquality, sPaginationType = "full_numbers")
data(airquality)
shiny::runApp()
shiny::runApp()
exoplanets <- read.csv("~/CDS/DDP/exoplanets/data/exoplanet_eu_catalog.csv")
str(exoplanets)
str(exoplanets$molecules)
exoplanets$molecules
str(exoplanets$molecules)
str(exoplanets)
exoplanets <- read.csv("~/CDS/DDP/exoplanets/data/exoplanet_eu_catalog.csv")
library(ggplot2)
library(plyr)
library(dplyr)
count(exoplanets$detection_type, "Astrometry")
count(exoplanets$detection_type, vars="Astrometry")
levels(exoplanets$detection_type)
count(exoplanets$detection_type)
?subset
?select
select(exoplanets, detection_type == "Imaging")
qplot(select(exoplanets, detection_type == "Imaging")$mass)
length(select(exoplanets, detection_type == "Imaging"))
length(select(exoplanets, detection_type == "Astrometry"))
?select
length(filter(exoplanets, detection_type == "Astrometry"))
astrometry <- filter(exoplanets, detection_type == "Astrometry")
?size
qplot(filter(exoplanets, detection_type == "Imaging")$mass)
qplot(filter(exoplanets, detection_type == "Astrometry")$mass)
qplot(filter(exoplanets, detection_type == "Radial velocity")$mass)
levels(exoplanets$detection_type)
qplot(filter(exoplanets, detection_type == "Radial Velocity")$mass)
qplot(filter(exoplanets, detection_type == "Microlensing")$mass)
count(exoplanets$detection_type)
qplot(filter(exoplanets, detection_type == "Transit")$mass)
shinyapps::setAccountInfo(name='gblazq', token='73956CBAED0EF94DFEF5C8D2D63C5868', secret='mqJf2Pz0vtlQyNjHnXgMwsWOKWp3ZIdCHTneJbX9')
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='gblazq', token='73956CBAED0EF94DFEF5C8D2D63C5868', secret='mqJf2Pz0vtlQyNjHnXgMwsWOKWp3ZIdCHTneJbX9')
library(shinyapps)
shinyapps::deployApp('~/CDS/DDP/particle_physics')
array_
array?
?array
?runif
array(c(1,0,2,-1,4,0.5), c(3,3))
x <- array(c(1,0,2,-1,4,0.5), c(3,3))
x[x > 1]
x > 1
shiny::runApp('~/CDS/DDP/particle_physics')
?checkboxInput
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
numeric("2")
as.vector("2")
as.numeric("2")
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
?ggplot
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
?plot.title
?theme
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
library(datasets)
sunspot.moth
srt(sunspot.month)
str(sunspot.month)
plot(sunspot.moth)
plot(sunspot.month)
?datasets
library(help = "datasets")
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
?read.table
shiny::runApp('~/CDS/DDP/particle_physics')
x <- read.table("data.dat")
x <- read.table("~/CDS/DDP/particle_physics/data.dat")
str(x)
hist(x)
View(x)
hist(x$V1)
?read.table
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
simData <- read.table("data.dat", col.names = c("mass"))
simData <- read.table("~/CDS/DDP/particle_physics/data.dat", col.names = c("mass"))
hist <- hist(simData$mass, breaks = 30)
binLength <<- hist$breaks[2]-hist$breaks[1]
data <- data.frame(breaks = hist$breaks[-length(hist$breaks)] + binLength/2,
counts = hist$counts,
sd = as.numeric(input$sigmas)*sqrt(hist$counts))
data <- data.frame(breaks = hist$breaks[-length(hist$breaks)] + binLength/2,
counts = hist$counts,
sd = as.numeric(2)*sqrt(hist$counts))
data <- data.frame(breaks = hist$breaks[-length(hist$breaks)] + binLength/2,
counts = hist$counts,
sd = as.numeric(1)*sqrt(hist$counts))
srt(data)
str(data)
normHist <- data
chisquared <- sum((normHist$counts - model(normHist$breaks, input$mean, input$f))^2/normHist$counts)
model <- function(x, mean, f){
m*binLength*(f*dnorm(x, mean, sd) +
(1-f)*rate/(exp(-min*rate)-exp(-max*rate))*exp(-x*rate))
}
chisquared <- sum((normHist$counts - model(normHist$breaks, input$mean, input$f))^2/normHist$counts)
m <- 10000
rate <- 1/1000
chisquared <- sum((normHist$counts - model(normHist$breaks, input$mean, input$f))^2/normHist$counts)
chisquared <- sum((normHist$counts - model(normHist$breaks, 5279, 0.5))^2/normHist$counts)
str(normHist$breaks)
chisquared <- sum((normHist$counts - model(normHist$breaks, 5279, 0.5))^2/normHist$counts)
traceback()
normHist$breaks
model(4325, 5279, 0.5)
traceback()
shiny::runApp('~/CDS/DDP/particle_physics')
traceback()
sd <- 100
chisquared <- sum((normHist$counts - model(normHist$breaks, 5279, 0.5))^2/normHist$counts)
shiny::runApp('~/CDS/DDP/particle_physics')
?coord_trans
shiny::runApp('~/CDS/DDP/particle_physics')
library(scales)
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
shiny::runApp('~/CDS/DDP/particle_physics')
chisquared <- sum((normHist$counts -
model(normHist$breaks, input$mean, input$f))^2/normHist$counts)
chisquared <- sum((normHist$counts -
model(normHist$breaks, 5279, 0.6))^2/normHist$counts)
min <- min(simData)
max <- max(simData)
chisquared <- sum((normHist$counts -
model(normHist$breaks, 5279, 0.6))^2/normHist$counts)
?chisq.test
chisq.test(normHist$counts, model(normHist$breaks, 5279, 0.6))
chisq.test(normHist$counts)
library(slidify)
library(slidifyLibraries)
setwd("~/CDS/DDP/DDP_Course_project")
setwd("~/CDS/DDP/slidify/DDP_Course_project")
ls
dir()
slidify("index.Rmd")
traceback()
slidify("index.Rmd")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
?fig.width
?fig.width
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
slidify("index.Rmd"); browseURL("index.html")
